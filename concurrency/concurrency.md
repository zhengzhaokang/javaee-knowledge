concurrency record

1、**进程和线程的区别?**

```
进程是系统进行资源分配和调度的独立单位，每一个进程都有它自己的内存空间和系统资源。
进程实现多处理机环境下的进程调度，分派，切换时，都需要花费较大的时间和空间开销。
为了提高系统的执行效率，减少处理机的空转时间和调度切换的时间，以及便于系统管理，所以有了线程，线程取代了
进程调度的基本功能。
简单来说，进程作为资源分配的基本单位，线程作为资源调度的基本单位。
```

2、**为什么要用多线程呢？**

```
使用多线程最主要的原因是提高系统的资源利用率。
现在CPU基本都是多核的，如果你只用单线程，那就是只用到了一个核心，其他的核心就相当于空闲在那里了。
在平时工作中多线程是随时都可见的。
比如说，我们系统Web服务器用的是Tomcat，Tomcat处理每一个请求都会从线程连接池里边用一个线程去处理。
又比如说，我们用连接数据库会用对应的连接池 Druid/C3P0/DBCP…
…等等这些都用了多线程的。
上面这些框架已经帮我们屏蔽掉「手写」多线程的问题
```

3、**实际开发中有用过吗？**

```
当然有了，在我所负责的系统也会用到多线程的。
比如跑一个定时任务，该任务的链路执行时间和过程都非常长，我这边就用一个线程池将该定时任务的请求进行处理。
这样做的好处就是可以及时返回结果给调用方，能够提高系统的吞吐量。

还有就是我的系统中用了很多生产者与消费者模式，会用多个线程去消费队列的消息，来提并发度。
```

4、**什么是线程安全？**

```
程安全就是多个线程去执行某类，这个类始终能表现出正确的行为，那么这个类就是线程安全的。

线程安全：多次运行，结果相同。
其实大部分时间我们在代码里边都没有显式去处理线程安全问题，因为这大部分都由框架所做了。
正如上面提到的Tomcat、Druid、SpringMVC等等。

很多时候，我们判断是否要处理线程安全问题，就看有没有多个线程同时访问一个共享变量。
像SpringMVC这种，我们日常开发时，不涉及到操作同一个成员变量，那我们就很少需要考虑线程安全问题。

我个人解决线程安全问题的思路有以下：
能不能保证操作的原子性，考虑atomic包下的类够不够我们使用
能不能保证操作的可见性，考虑volatile够不够我们使用
如果涉及到对线程的控制，比如一次能使用多少个，当前的触发条件是否依赖其他线程的结果，考虑countDownLatch/Semaphore等等.
如果是集合，考虑java.util.concurrent包下的集合类。
如果synchronized类满足不了，考虑lock包下的类.

就是先判断有没有线程安全问题，如果存在则根据具体的情况去判断使用什么方式去处理线程安全的问题。
虽然synchronized很牛逼，但无脑使用synchronized会影响我们程序的性能的。
```

5、**什么情况会造成死锁？**

```
造成死锁的原因可以简单概括为：当前线程拥有其他线程需要的资源，当前线程等待其他线程已拥有的资源，
都不放弃自己拥有的资源。

死锁是指两个或多个进程（或线程）在执行过程中，因争夺资源而造成的一种相互等待的现象，若无外力作用，
这些进程（或线程）都将无法向前推进。

避免死锁的方式一般有以下方案：
1、固定加锁的顺序，比如我们可以使用hash值的大小来确定加锁的先后。
2、尽可能缩减加锁的范围，等到操作共享变量的时候才加锁。
3、使用可释放的定时锁（一段时间申请不到锁的权限了，直接释放掉）

死锁的发生通常需要满足以下四个必要条件，这四个条件也被称为死锁的四个基本要素：
互斥条件：资源每次只能被一个进程（或线程）使用，即资源是独占的。
请求与保持条件：一个进程（或线程）因请求资源而阻塞时，对已获得的资源保持不放。
不可剥夺条件：进程（或线程）已获得的资源，在未使用完之前，不能强行剥夺。
循环等待条件：若干进程（或线程）之间形成一种头尾相接的循环等待资源关系。
```

6、**聊聊CAS吧**

```
CAS的全称为compare and swap，比较并交换，但它是一个原子性的操作，对应到CPU指令为cmpxchg

CAS 有三个操作数：当前值A、内存值V、要修改的新值B。
假设 当前值A 跟 内存值V 相等，那就将 内存值V 改成B；假设 当前值A 跟 内存值V 不相等，要么就重试，要么就放弃更新。
将当前值与内存值进行对比，判断是否有被修改过，这就是CAS的核心。

synchronized锁每次只会让一个线程去操作共享资源，
而CAS相当于没有加锁，多个线程都可以直接操作共享资源，在实际去修改的时候才去判断能否修改成功。

CAS有个缺点就是会带来ABA的问题。
假设线程A读到当前值是10，可能线程B把值修改为100，然后线程C又把值修改为10。
等到线程A拿到执行权时，因为当前值和内存值是一致的，线程A是可以修改的！
要解决ABA的问题，Java也提供了AtomicStampedReference类供我们用，说白了就是加了个版本，比对的就是内存值+版本是否一致。
```

7、**LongAdder 对象比tomicLong 性能更好吗？**

```
LongAdder 对象在大多数并发场景下确实比 AtomicLong 提供了更好的性能。这主要是因为它们采用了不同的内部实现策略来处理并发更新。

AtomicLong
AtomicLong 使用了基于 CAS（Compare-And-Swap）操作的 volatile long 变量来实现原子性。
当多个线程频繁更新同一个 AtomicLong 实例时，CAS 操作可能会因为竞争而导致频繁的失败和重试，这在高并发场景下可能会影响性能。
LongAdder
LongAdder 是 Java 8 引入的一个类，旨在作为 AtomicLong 的高性能替代，特别是在高并发场景下。
它通过内部维护多个变量来减少竞争，每个线程（或 CPU）可能会更新它自己的计数器（称为“单元格”或“分段”），而不是所有线程都竞争同一个变量。
最终，这些内部计数器的值会被合并（累加）以得到总数，这个过程可以是在需要时（比如调用 sum() 方法时）以较低的频率进行，从而减少了高并发时的竞争。
LongAdder 还提供了更好的可伸缩性，因为它能够随着线程数量的增加而增加内部计数器的数量，从而保持较好的性能。
何时使用 LongAdder
当预期会有大量的并发更新时，LongAdder 通常比 AtomicLong 提供更好的性能。
如果你不需要非常严格的即时可见性（即，你不需要在每次更新后立即看到最新值），那么 LongAdder 是一个不错的选择。
然而，如果你需要频繁地读取当前值（比如在一个高频率的循环中），那么 AtomicLong 可能会是更好的选择，因为 LongAdder 的 sum() 
方法会遍历所有内部计数器来计算总和，这可能会引入一些开销。
结论
总的来说，LongAdder 在高并发场景下通常比 AtomicLong 提供更好的性能，但选择哪个取决于你的具体需求。如果你需要处理大量的并发更新，
并且不需要非常严格的即时可见性，那么 LongAdder 是一个很好的选择。否则，如果你需要频繁地读取当前值，或者对内存占用和延迟有严格要求，那么 AtomicLong 可能更适合你的需求。
```

8、**聊聊synchronized**

```
synchronized是一种互斥锁，一次只能允许一个线程进入被锁住的代码块。
synchronized是Java的一个关键字，它能够将代码块/方法锁起来。
如果synchronized修饰的是实例方法，对应的锁则是对象实例。
如果synchronized修饰的是静态方法，对应的锁则是当前类的Class实例。
如果synchronized修饰的是代码块，对应的锁则是传入synchronized的对象实例。
```

9、**synchronized的原理**

```
通过反编译可以发现，当修饰方法时，编译器会生成 ACC_SYNCHRONIZED 关键字用来标识
当修饰代码块时，会依赖monitorenter和monitorexit指令
但前面已经说了，无论synchronized修饰的是方法还是代码块，对应的锁都是一个实例（对象）
在内存中，对象一般由三部分组成，分别是对象头、对象实际数据和对齐填充
重点在于对象头，对象头又由几部分组成，但我们重点关注对象头Mark Word的信息就好了
Mark Word会记录对象关于锁的信息
又因为每个对象都会有一个与之对应的monitor对象，monitor对象中存储着当前持有锁的线程以及等待锁的线程队列
了解Mark Word和monitor对象是理解 synchronized 原理的前提
```

10、synchronized锁在 JDK 1.6 之后做了很多的优化，这块了解多少

```
在JDK 1.6之前是重量级锁，线程进入同步代码块/方法 时
monitor对象就会把当前进入线程的Id进行存储，设置MarkWord的monitor对象地址，并把阻塞的线程存储到monitor的等待线程队列中
它加锁是依赖底层操作系统的 mutex 相关指令实现，所以会有用户态和内核态之间的切换，性能损耗十分明显

而JDK1.6 以后引入偏向锁和轻量级锁在JVM层面实现加锁的逻辑，不依赖底层操作系统，就没有切换的消耗
所以，Mark Word对锁的状态记录一共有4种：无锁、偏向锁、轻量级锁和重量级锁
```

11、synchronized 偏向锁、轻量级锁和重量级锁

```
偏向锁指的就是JVM会认为只有某个线程才会执行同步代码（没有竞争的环境）

所以在Mark Word会直接记录线程ID，只要线程来执行代码了，会比对线程ID是否相等相等则当前线程能直接获取得到锁，执行同步代码
如果不相等，则用CAS来尝试修改当前的线程ID，如果CAS修改成功，那还是能获取得到锁，执行同步代码

如果CAS失败了，说明有竞争环境，此时会对偏向锁撤销，升级为轻量级锁。
在轻量级锁状态下，当前线程会在栈帧下创建Lock Record，LockRecord 会把Mark Word的信息拷贝进去，且有个0wner指针指向加锁的对象
线程执行到同步代码时，则用CAS试图将Mark Word的指向到线程栈帧的Lock Record，假设CAS修改成功，则获取得到轻量级锁

假设修改失败，则自旋(重试)，自旋一定次数后，则升级为重量级锁
（在一些JVM版本中，这个默认值可能是10次，但这并不是一个绝对固定的数字。JVM可能会根据系统的运行状况、CPU的负载情况等因素
来动态调整这个自旋次数，以达到最优的性能。）

简单总结一下：
synchronized锁原来只有重量级锁，依赖操作系统的mutex指令，需要用户态和内核态切换，性能损耗十分明显
重量级锁用到monitor对象，而偏向锁则在Mark Word记录线程ID进行比对，
轻量级锁则是拷贝Mark Word到Lock Record，用CAS+自旋的方式获取。
偏向锁则是CAS对比Mark Word ID

引入了偏向锁和轻量级锁，就是为了在不同的使用场景使用不同的锁，进而提高效率。锁只有升级，没有降级。
1)只有一个线程进入临界区+，偏向锁
2)多个线程交替进入临界区，轻量级锁
3)多线程同时进入临界区，重量级锁
```

12、**聊聊lock锁**

```
什么叫做公平和非公平锁
公平锁指的就是：在竞争环境下，先到临界区的线程比后到的线程一定更快地获取得到锁
那非公平就很好理解了：先到临界区的线程未必比后到的线程更快地获取得到锁

如果让你实现的话，你怎么实现公平和非公平锁？
公平锁可以把竞争的线程放在一个先进先出的队列上
只要持有锁的线程执行完了，唤醒队列的下一个线程去获取锁就好了
非公平锁的概念上面已经提到了：后到的线程可能比前到临界区的线程获取得到锁
那实现也很简单，线程先尝试能不能获取得到锁，如果获取得到锁了就执行同步代码了
如果获取不到锁，那就再把这个线程放到队列呗。
所以公平和非公平的区别就是：线程执行同步代码块时，是否会去尝试获取锁。
如果会尝试获取锁，那就是非公平的。如果不会尝试获取锁，直接进队列，再等待唤醒，那就是公平的。

为什么要进队列呢？线程一直尝试获取锁不就行了么？
一直尝试获取锁，专业点就叫做自旋，需要耗费资源的。
多个线程一直在自旋，而且大多数都是竞争失败的，哪有人会这样实现的

synchronized锁是公平的还是非公平的？
非公平的。
偏向锁很好理解，如果当前线程ID与markword存储的不相等，则CAS尝试更换线程ID，CAS成功就获取得到锁了
CAS失败则升级为轻量级锁
轻量级锁实际上也是通过CAS来抢占锁资源(只不过多了拷贝Mark Word到Lock Record的过程)
抢占成功到锁就归属给该线程了，但自旋失败一定次数后升级重量级锁
重量级锁通过monitor对象中的队列存储线程，但线程进入队列前，还是会先尝试获取得到锁，如果能获取不到才进入线程等待队列中

综上所述，synchronized无论处理哪种锁，都是先尝试获取，获取不到才升级放到队列上的，所以是非公平的
```

13、**AQS你了解吗**

```
AQS全称叫做AbstractQueuedSynchronizer
是可以给我们实现锁的一个「框架」，内部实现的关键就是维护了一个先进先出的队列以及state状态变量

先进先出队列存储的载体叫做Node节点，该节点标识着当前的状态值、是独占还是共享模式以及它的前驱和后继节点等等信息
简单理解就是：AQS定义了模板，具体实现由各个子类完成。
总体的流程可以总结为：会把需要等待的线程以Node的形式放到这个先进先出的队列上，state变量则表示为当前锁的状态。

像ReentrantLock、ReentrantReadWriteLock.CountDownLatch.:Semaphore这些常用的实现类都是基于AQS实现的
AQS支持两种模式：独占（锁只会被一个线程独占）和共享（多个线程可同时执行）
```

13、**ReentrantLock来讲讲加锁和解锁的过程呗**

```
以非公平锁为了，我们在外界调用lock方法的时候，源码是这样实现的
1):CAS尝试获取锁，获取成功则可以执行同步代码
2):CAS获取失败，则调用acquire方法，acquire方法实际上就是AQS的模板方法
3):acquire首先会调用子类的tryAcquire方法(又回到了ReentrantLock中)
4):tyAcquire方法实际上会判断当前的state是否等于0，等于0说明没有线程持有锁，则又尝试CAS直接获取锁
5):如果CAS获取成功，则可以执行同步代码
6):如果CAS获取失败，那判断当前线程是否就持有锁，如果是持有的锁，那更新state的值，获取得到锁(这里其实就是处理可重入的逻辑)
7):CAS失败&&非重入的情况，则回到tryAcquire方法执行「入队列」的操作
8):将节点入队列之后，会判断「前驱节点」是不是头节点，如果是头结点+又会用CAS尝试获取锁
9):如果是「前驱节点」是头节点并获取得到锁，则把当前节点设置为头结点，并且将前驱节点置空（实际上就是原有的头节点已经释放锁了）
10）当线程CAS获取锁失败，将当前线程入队列，把前驱节点状态设置为SIGNAL状态，并将自己挂起。
11):最后调用park将当前线程挂起


为什么要设置前驱节点为SIGNAL状态？
其实归终结底就是为了判断节点的状态，去做些处理。
Node 中节点的状态有4种，分别是:CANCELLED(1)、SIGNAL(-1)、CONDITION(-2)PROPAGATE(-3)和0
在ReentrantLock解锁的时候，会判断节点的状态是否小于0，小于等于0才说明需要被唤醒。
另外一提的是:公平锁的实现与非公平锁是很像的，只不过在获取锁时不会直接尝试使用CAS来获取锁。
只有当队列没节点并且state为0时才会去获取锁，不然都会把当前线程放到队列中。
```

14、**为什么需要线程池？**

```
JVM在HotSpot的线程模型下，Java线程会一对一映射为内核线程
这意味着，在Java中每次创建以及回收线程都会去内核创建以及回收
这就有可能导致：创建和销毁线程所花费的时间和资源可能比处理的任务花费的时间和资源要更多

线程池的出现是为了提高线程的复用性以及固定线程的数量！！！
```

15、**那你是怎么用线程池的呢？用 Executors 去创建的吗？**

```
不是的，我这边用的ThreadPoolExecutor去创建线程池
其实看阿里巴巴开发手册就有提到，不要使用Executors去创建线程。
最主要的目的就是:使用ThreadPoolExecutor创建的线程你是更能了解线程池运行的规则，避免资源耗尽的风险
ThreadPoolExecutor在构造的时候有几个重要的参数，分别是:
corePoolSize(核心线程数量)、maximumPoolSize(最大线程数量)、keepAliveTime(线程空余时间)、workQueue(阻队列)、handler(任务拒绝策略)
这几个参数应该很好理解哈，我就说下任务提交的流程，分别对应着几个参数的作用吧
1):首先会判断运行线程数是否小于corePoolSize，如果小于，则直接创建新的线程执行任务
2):如果大于corePoolSize，判断workQueue阻塞队列是否已满，如果还没满，则将任务放到阻塞队列中
3):如果workQueue阻塞队列已经满了，则判断当前线程数是否大于maximumPoolSize，如果没大于则创建新的线程执行任务
4):如果大于maximumPoolSize，则执行任务拒绝策略(具体就是你自己实现的handler)
这里有个点需要注意下，就是workQueue阻塞队列满了，但当前线程数小于maximumPoolSize，这时候会创建新的线程执行任务
源码就是这样实现的

不过一般我们都是将corePoolSize和maximumPoolSize设置相同数量。
keepAliveTime指的就是，当前运行的线程数大于核心线程数了，只要空闲时间达到了，就会对线程进行回收。
```

16、**你创建线程池肯定会指定线程数的嘛，你这块是怎么考量的**。

```
线程池指定线程数这块，首先要考量自己的业务是什么样的
是cpu密集型的还是io密集型的，假设运行应用的机器CPU核心数是N
那cpu密集型的可以先给到N+1，io密集型的可以给到2N去试试
上面这个只是一个常见的经验做法，具体究竟开多少线程，需要压测才能比较准确地定下来
线程不是说越大越好，在之前的面试我也提到过，多线程是为了充分利用CPU的资源
如果设置的线程过多，线程大量有上下文切换，这一部分也会带来系统的开销，这就得不偿失了
```

17、**工作中有用到过ThreadLocal吗？**

```
Spring提供了事务相关的操作，而我们知道事务是得保证一组操作同时成功或失败的
这意味着我们一次事务的所有操作需要在同一个数据库连接上
但是在我们日常写代码的时候是不需要关注这点的
Spring就是用的ThreadLocal来实现，ThreadLocal存储的类型是一个Map
Map中的key 是DataSource，value 是Connection（为了应对多数据源的情况，所以是一个Map）
用了ThreadLocal保证了同一个线程获取一个Connection对象，从而保证一次事务的所有操作需要在同一个数据库连接上
```

18、**ThreadLocal内存泄露?**

```
ThreadLocal是一个壳子，真正的存储结构+是ThreadLocal里有ThreadLocalMap这么个内部类
而有趣的是，ThreadLocalMap的引用是在Thread上定义的
ThreadLocal本身并不存储值，它只是作为key来让线程从ThreadLocalMap获取value
所以，得出的结论就是ThreadLocalMap该结构本身就在Thread下定义，而ThreadLocal只是作为key，
存储set到ThreadLocalMap的变量当然是线程私有的咯

使用ThreadLocal的最佳实践就是：用完了，手动remove掉。就像使用Lock加锁后，要记得解锁

什么叫做内存泄露？
意思就是：你申请完内存后，你用完了但没有释放掉，你自己没法用，系统又没法回收。

总结：
什么是ThreadLocal:它提供了线程的局部变量，每个线程都可以通过set/get来对这个局部变量
进行操作，不会和其他线程的局部变量进行冲突，实现了线程的数据隔离。ThreadLocal实际用处(举例):Spring事务，
ThreadLocal里存储Map，Key是DataSourceValue是Connection
ThreadLocal设计:Thread有ThreadLocalMap引用，ThreadLoca作为ThreadLocalMap的Keyset和get进去的Value则
是ThreadLocalMap的value
ThreadLocal内存泄露:ThreadLocal被回收&&线程被复用&&线程复用后不再调用ThreadLocal的set/getremove方法 
才可能发生内存泄露(条件还是相对苛刻)ThreadLocal最佳实践:用完就要remove掉
```

19、**现在我有50个任务，这50个任务在完成之后，才能执行下一个「函数」，要是你，你怎么设计？**

```
可以用JDK给我们提供的线程工具类，CountDownLatch和CyclicBarrier都可以完成这个需求。
这两个类都可以等到线程完成之后，才去执行某些操作

CountDownLatch和CyclicBarrier有什么什么区别呢

1.CountDownLatch和CyclicBarrier都是线程同步的工具类，都是基于AQS实现的；
2.CountDownLatch 的计数器是大于或等于线程数的，而CyclicBarrier是一定等于线程数；
3.CountDownLatch 放行由其他线程控制而CyclicBarrier是由本身来控制的，CountDownLatch允许一个或多个线程一直等待，
直到这些线程完成它们的操作，而CyclicBarrier不一样，它往往是当线程到达某状态后，暂停下来等待其他线程，
等到所有线程均到达以后，才继续执行，二者的等待主体是不一样的；
4.CountDownLatch调用await()通常是主线程/调用线程，而CyclicBarrier调用await()是在任务线程调用的；
5.CountDownLatch 操作的是事件，阻塞足够多的次数即可，不管几个线程；而 CyclicBarrier 侧重点是线程，
强调多个线程间互相等待，同时结束；
6.CountDownLatch 是不可以重置的，所以无法重用；而 CyclicBarrier 则没有这个限制，可以重用；
```

代码FYI

```
publicclassCyclicBarrierTest{
	privatefinalstaticExecutorServiceEXECUTOR_SERVICE=Executors.newFixedThreadPool(5);
	privatefinalstaticCyclicBarrierBARRIER=newCyclicBarrier(10);
	
	publicstaticvoidmain(String[]args){
		for(inti=0;i<5;i++){
			finalStringname="玩家"+i;
			
			EXECUTOR_SERVICE.execute(newRunnable(){
			
				@Override
				publicvoidrun(){
					try{
						Thread.sleep(2000);
						System.out.println(name+"已准备,等待其他玩家准备...");
						BARRIER.await();
						Thread.sleep(1000);
						System.out.println(name+"已加入游戏");
					}catch(InterruptedExceptione){
						System.out.println(name+"离开游戏");
					}catch(BrokenBarrierExceptione){
						System.out.println(name+"离开游戏");
					}
				}
			});
		}
		EXECUTOR_SERVICE.shutdown();
	}
}
```

20、**聊聊Java内存模型**

```
嗯，我简单说下我的理解吧。那我就从为什么要有Java内存模型开始讲起吧

1.现有计算机往往是多核的，每个核心下会有高速缓存。高速缓存的诞生是由于「CPU与内存(主存)的速度存在差异」，
L1和L2缓存一般是「每个核心独占」一份的。

2.为了让CPU提高运算效率，处理器可能会对输入的代码进行「乱序执行+」，也就是所谓的「指令重排序」

3.一次对数值的修改操作往往是非原子性的(比如i++实际上在计算机执行时就会分成多个指令)

在永远单线程下，上面所讲的均不会存在什么问题，因为单线程意味着无并发。并且在单线程下，编译器/runtime/处理器
都必须遵守as-if-serial语义，遵守as-if-serial意味着它们不会对「数据依赖关系的操作」做重排序。

CPU为了效率，有了高速缓存、有了指令重排序等等，整块架构都变得复杂了。我们写的程序肯定也想要「充分」利用CPU的资源啊!
于是乎，我们使用起了多线程

多线程在意味着并发，并发就意味着我们需要考虑线程安全问题
1.缓存数据不一致:多个线程同时修改「共享变量」，CPU核心下的高速缓存是「不共亨」的，
那多个cache与内存之间的数据同步该怎么做?
2.CPU指令重排序在多线程下会导致代码在非预期下执行，最终会导致结果存在错误的情况。

针对于「缓存不一致」问题，CPU也有其解决办法，常被大家所认识的有两种:
1.使用「总线锁」:某个核心在修改数据的过程中，其他核心均无法修改内存中的数据。
(类似于独占内存的概念，只要有CPU在修改，那别的CPU就得等待当前CPU释放)
2.缓存一致性协议(MESI协议，其实协议有很多，只是举个大家都可能见过的)。MESI拆开英文是(Modified (修改状态)、
Exclusive(独占状态)、Share(共享状态)、Invalid(无效状态))
缓存一致性协议我认为可以理解为「缓存锁」，它针对的是「缓存行」(Cache line) 进行加锁”，
所谓「缓存行」其实就是 高速缓存 存储的最小单位。

MESI协议的原理大概就是:当每个CPU读取共享变量之前，会先识别数据的「对象状态(是修改、还是共享、还是独占、还是无效)。
如果是独占，说明当前CPU将要得到的变量数据是最新的，没有被其他CPU所同时读取
如果是共享，说明当前CPU将要得到的变量数据还是最新的，有其他的CPU在同时读取但还没被修改
如果是修改，说明当前CPU正在修改该变量的值，同时会向其他CPU发送该数据状态为invalid(无效)的通知，
得到其他CPU响应后(其他CPU将数据状态从共享(share)变成invalid(无效))，会当前CPU将高速缓存的数据写到主存，
并把自己的状态从modify(修改)变成exclusive(独点)

如果是无效，说明当前数据是被改过了，需要从主存重新读取最新的数据

其实MESI协议做的就是判断「对象状态」，根据「对象状态」做不同的策略。关键就在于某个CPU在对数据进行修改时，
需要「同步」通知其他CPU，表示这个数据被我修改了，你们不能用了。

比较于「总线锁」，MESI协议的”锁粒度”更小了，性能那肯定会更高咯

但据我了解，CPU还有优化，你还知道吗?
嗯，还是了解那么一点点的。
从前面讲到的，可以发现的是:当CPU修改数据时，需要「同步」告诉其他的CPU，等待其他CPU响应接收到invalid(无效)后，
它才能将高速缓存数据写到主存。
同步，意味着等待，等待意味着什么都干不了。CPU肯定不乐意啊，所以又优化了一把。
优化思路就是从「同步」变成「异步」。
在修改时会「同步」告诉其他CPU，而现在则把最新修改的值写到[store bufer!中，并通知其他CPU记得要改状态，
随后CPU就直接返回干其他事了，等到收到其它CPU发过来的响应消息，再将数据更新到高速缓存中。
其他CPU接收到invalid(无效)通知时，也会把接收到的消息放入「invalid queue」中，只要写到[invalid queue」
就会直接返回告诉修改数据的CPU已经将状态置为「[invalid」

异步化会导致「CPU乱序执行」/「缓存不及时可见」问题
通过内存屏障解决。就是把「异步优化」给”禁用“掉
```

![img](https://picx.zhimg.com/80/v2-5ff4ed77200deca0f2be416b9b351da0_1440w.png?source=d16d100b)





JAVA内存模型1

![img](https://picx.zhimg.com/80/v2-9f525c2479b5f0128c4760ba8f14b3d2_1440w.png?source=d16d100b)





JAVA内存模型2

```
Java内存模型定义了8种操作来完成「变量如何从主内存到本地内存，以及变量如何从本地内存到主内存」
分别是read/load/use/assign/store/write/lock/unlock操作
```